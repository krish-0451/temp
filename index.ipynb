{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "def period_stat(df, period=20, volume_multiplier=2):\n",
    "    df['High'] = pd.to_numeric(df['High'], errors='coerce')\n",
    "    df['Low'] = pd.to_numeric(df['Low'], errors='coerce')\n",
    "    df['Close'] = pd.to_numeric(df['Close'], errors='coerce')\n",
    "    df['Open'] = pd.to_numeric(df['Open'], errors='coerce')\n",
    "    df['Volume'] = pd.to_numeric(df['Volume'], errors='coerce')\n",
    "\n",
    "    # Drop rows with NaN values in the relevant columns\n",
    "    df.dropna(subset=['High', 'Low', 'Close','Open','Volume'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Compute the required metrics\n",
    "    df[f'{period}_Max%'] = (df['High'].rolling(window=period).max() * 100) / df['Close']  # Max% relative to Close\n",
    "    df[f'{period}_Min%'] = (df['Low'].rolling(window=period).min() * 100) / df['Close']   # Min% relative to Close\n",
    "    df[f'{period}_Range%'] = df[f'{period}_Max%'] - df[f'{period}_Min%']  # Range% relative to Close\n",
    "    df[f'{period}_Avg_Price%'] = ((df[f'{period}_Max%'] + df[f'{period}_Min%']) / 2)  # Avg Price% relative to Close\n",
    "    \n",
    "    # Price Distribution Metrics\n",
    "    df[f'{period}_Std%'] = df['Close'].rolling(window=period).std() * 100 / df['Close']  # Standard Deviation in % terms\n",
    "    df[f'{period}_Skew'] = df['Close'].rolling(window=period).apply(lambda x: skew(x))  # Skewness\n",
    "    df[f'{period}_Kurtosis'] = df['Close'].rolling(window=period).apply(lambda x: kurtosis(x))  # Kurtosis\n",
    "    df[f'{period}_IQR%'] = (df['Close'].rolling(window=period).apply(lambda x: np.percentile(x, 75) - np.percentile(x, 25)) / df['Close']) * 100  # IQR in percentage terms\n",
    "    df[f'{period}_CV'] = df[f'{period}_Std%'] / df['Close'] * 100  # Coefficient of Variation in percentage\n",
    "    df[f'{period}_Range_to_Std'] = df[f'{period}_Range%'] / df[f'{period}_Std%']  # Range to Standard Deviation ratio\n",
    "    df[f'{period}_Variance'] = df['Close'].rolling(window=period).var() * 100 / df['Close']  # Variance in percentage terms\n",
    "\n",
    "    # Trend Change and Reversal Metrics\n",
    "    # Down days and up days percentage\n",
    "    df[f'{period}Up_Day'] = (df['Close'] > df['Open']).astype(int)  # 1 for Up day, 0 for Down day\n",
    "    df[f'{period}Down_Day'] = (df['Close'] < df['Open']).astype(int)  # 1 for Down day, 0 for Up day\n",
    "\n",
    "    df[f'{period}_Up_Days_Percentage'] = df[f'{period}Up_Day'].rolling(window=period).mean() * 100  # % of Up Days in last 'period'\n",
    "    df[f'{period}_Down_Days_Percentage'] = df[f'{period}Down_Day'].rolling(window=period).mean() * 100  # % of Down Days in last 'period'\n",
    "\n",
    "    # Consecutive Up and Down Days\n",
    "    def consecutive_days(arr, trend_type):\n",
    "        count, max_count = 0, 0\n",
    "        for value in arr:\n",
    "            if value == trend_type:\n",
    "                count += 1\n",
    "                max_count = max(max_count, count)\n",
    "            else:\n",
    "                count = 0\n",
    "        return max_count\n",
    "    \n",
    "    # Apply consecutive day calculation for Up and Down trends\n",
    "    df[f'{period}_Consecutive_Up_Days'] = df[f'{period}Up_Day'].rolling(window=period).apply(lambda x: consecutive_days(x, 1))\n",
    "    df[f'{period}_Consecutive_Down_Days'] = df[f'{period}Down_Day'].rolling(window=period).apply(lambda x: consecutive_days(x, 1))\n",
    "\n",
    "    # Market Movement Cycle Acceleration\n",
    "    df[f'{period}_Market_Movement_Acceleration%'] = (((df['Close'] - 2 * df['Close'].shift(1) + df['Close'].shift(period)) / (1 ** 2)) / df['Close']) * 100\n",
    "\n",
    "    # Additional Candle Metrics\n",
    "    df[f'{period}_Avg_Candle_Size%'] = ((df['High'] - df['Low']).rolling(window=period).mean() / df['Close']) * 100  # Average Candle Size in percentage\n",
    "    df[f'{period}_Candle_Size_Std%'] = ((df['High'] - df['Low']).rolling(window=period).std() / df['Close']) * 100  # Std of Candle Size in %\n",
    "\n",
    "    # Candle Body to Wick Ratio\n",
    "    df['Candle_Body'] = abs(df['Close'] - df['Open'])\n",
    "    df['Candle_Wick_Upper'] = df['High'] - df[['Close', 'Open']].max(axis=1)\n",
    "    df['Candle_Wick_Lower'] = df[['Close', 'Open']].min(axis=1) - df['Low']\n",
    "    df[f'{period}_Candle_Body_to_Wick_Ratio'] = df['Candle_Body'] / (df['Candle_Wick_Upper'] + df['Candle_Wick_Lower'])\n",
    "\n",
    "    # Change per Candle (in percentage)\n",
    "    df[f'{period}_Change_Per_Candle_%'] = (df['Close'] - df['Open']) / df['Open'] * 100\n",
    "\n",
    "    # Additional Requested Metrics\n",
    "\n",
    "    # Divergence Percentage: Close vs SMA\n",
    "    df[f'{period}_Divergence_Percentage'] = ((df['Close'] - df['Close'].rolling(window=period).mean()) / df['Close']) * 100\n",
    "\n",
    "    # Average Slope\n",
    "    def average_slope_percentage(x):\n",
    "        y = x.values\n",
    "        x = np.arange(len(x))\n",
    "        slope = np.polyfit(x, y, 1)[0]  # Linear regression to find slope\n",
    "        return slope / y[-1] * 100  # Normalize slope to percentage of the last closing price\n",
    "\n",
    "    df[f'{period}_Avg_Slope%'] = df['Close'].rolling(window=period).apply(average_slope_percentage)  # Average Slope in percentage terms\n",
    "\n",
    "\n",
    "    # Average Bullish Candle Ratio and Average Bearish Candle Ratio\n",
    "    df[f'{period}_Avg_Bullish_Candle_Ratio'] = df[f'{period}Up_Day'].rolling(window=period).mean() * 100\n",
    "    df[f'{period}_Avg_Bearish_Candle_Ratio'] = df[f'{period}Down_Day'].rolling(window=period).mean() * 100\n",
    "\n",
    "    # Average Volume %\n",
    "    df[f'{period}_Avg_Volume_%'] = (df['Volume'] / df['Volume'].rolling(window=period).mean()) * 100\n",
    "\n",
    "    # Volume Spike %\n",
    "    df[f'{period}_Volume_Spike_%'] = (df['Volume'] / df['Volume'].rolling(window=period).mean()) * 100\n",
    "    df[f'{period}_Volume_Spike_%'] = df[f'{period}_Volume_Spike_%'].where(df[f'{period}_Volume_Spike_%'] > volume_multiplier * 100, 0)\n",
    "\n",
    "    # Close Lag Percentage\n",
    "    df[f'{period}_Close_Lag_1%'] = ((df['Close'] - df['Close'].shift(1)) / df['Close']) * 100\n",
    "    df[f'{period}_Close_Lag_5%'] = ((df['Close'] - df['Close'].shift(5)) / df['Close']) * 100\n",
    "    df[f'{period}_Close_Lag_20%'] = ((df['Close'] - df['Close'].shift(20)) / df['Close']) * 100\n",
    "\n",
    "    # Day, Week of the Day, Month, Quarter\n",
    "    df['Day_of_Week'] = df.index.dayofweek  # 0 = Monday, 6 = Sunday\n",
    "    df['Week_of_Year'] = df.index.isocalendar().week\n",
    "    df['Month'] = df.index.month\n",
    "    df['Quarter'] = df.index.quarter\n",
    "\n",
    "    # Net Change Percentage (Open of First Row and Close of Last Row of the Period)\n",
    "    df[f'{period}_Net_Change_%'] = ((df['Close'] - df['Open'].shift(period - 1)) / df['Open'].shift(period - 1)) * 100\n",
    "\n",
    "    # Cumulative Return Percentage\n",
    "    df[f'{period}_Cumulative_Return_%'] = ((df['Close'] / df['Close'].shift(period - 1)) - 1) * 100\n",
    "\n",
    "    # Strength Metric: (Close(period) - Close(1)) / (Max - Min)\n",
    "    df[f'{period}_Strength'] = (df['Close'] - df['Close'].shift(period - 1)) / (df[f'{period}_Max%'] - df[f'{period}_Min%'])\n",
    "\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,precision_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "# file_path = \"your_file.csv\"  # Update with your actual file path\n",
    "data['tar'] = (data['Open'].shift(-1) > data['Close']).astype(int)\n",
    "df = data\n",
    "\n",
    "# Ensure your data is cleaned and processed\n",
    "df_numeric = df.replace([np.inf, -np.inf], np.nan)\n",
    "df_numeric = df_numeric.dropna(subset=['tar'])\n",
    "\n",
    "# Define features and target (Using all features except the target)\n",
    "target_col = \"tar\"\n",
    "X = df_numeric.drop(columns=[target_col])\n",
    "y = df_numeric[target_col]\n",
    "\n",
    "# Handle missing values (optional: you can refine this based on data)\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "model = RandomForestClassifier(n_estimators=5, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Train the model on all features\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = precision_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy with All Features: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,precision_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "# file_path = \"your_file.csv\"  # Update with your actual file path\n",
    "data['tar'] = (data['Open'].shift(-1) > data['Close']).astype(int)\n",
    "df = data\n",
    "\n",
    "# Ensure your data is cleaned and processed\n",
    "df_numeric = df.replace([np.inf, -np.inf], np.nan)\n",
    "df_numeric = df_numeric.dropna(subset=['tar'])\n",
    "\n",
    "# Define features and target\n",
    "target_col = \"tar\"\n",
    "X = df_numeric.drop(columns=[target_col])\n",
    "y = df_numeric[target_col]\n",
    "\n",
    "# Handle missing values (optional: you can refine this based on data)\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Initialize the model\n",
    "model = RandomForestClassifier(n_estimators=5, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Perform Forward Feature Selection using mlxtend\n",
    "sfs = SFS(model,\n",
    "          k_features=\"best\",    # You can use 'best' or specify a number of features\n",
    "          forward=False,         # Forward selection\n",
    "          floating=False,       # No floating\n",
    "          verbose=5,            # To see the progress\n",
    "          scoring='precision',   # Metric to evaluate the selected features\n",
    "          cv=3,                 # Cross-validation splitting\n",
    "          n_jobs=-1)\n",
    "\n",
    "# Fit the feature selector\n",
    "sfs.fit(X_scaled, y)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features = X.columns[list(sfs.k_feature_idx_)]\n",
    "print(\"Selected Features:\", list(selected_features))\n",
    "\n",
    "# Train model on selected features\n",
    "X_selected = X_scaled[:, list(sfs.k_feature_idx_)]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = precision_score(y_test, y_pred)\n",
    "print(f\"Final Model Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
